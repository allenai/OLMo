run_name: "mup-test"
precision: "fp32"
save_folder: "/tmp/olmo-train-tiny"
model:
  d_model: 16
  n_heads: 2
  n_layers: 1
  mlp_ratio: 1
  alibi: false
  rope: true
  alibi_bias_max: 8.0
  attention_dropout: 0.1
  attention_layer_norm: false
  residual_dropout: 0.1
  embedding_dropout: 0.1
  max_sequence_length: 2048
  vocab_size: 50257
  eos_token_id: 50256
  pad_token_id: 50256
  init_device: null
  init_fn: "normal"
  init_std: 0.02
optimizer:
  name: adamw
  learning_rate: 3.0e-4
  weight_decay: 0.1
  eps: 1e-8
  decay_norm_and_bias: true
  decay_embeddings: false
  betas:
  - 0.9
  - 0.95
  metrics_log_interval: 1

scheduler:
  name: "cosine_with_warmup"
  t_warmup: 10
data:
  paths:
    - "/net/nfs.cirrascale/allennlp/llm-data/c4/en/c4-train.00000-00099.npy"
  persistent_workers: false
  num_workers: 0
  prefetch_factor: null
tokenizer:
  identifier: "gpt2"
save_overwrite: true
max_duration: 4
global_train_batch_size: 4
device_train_microbatch_size: 4
