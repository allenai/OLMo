name: olmo-70b
image: mosaicml/pytorch:2.2.1_cu121-python3.11-ubuntu20.04
scheduling:
  priority: auto
  # preemptible: true  # means it can be retried
  # max_retries: 3
compute:
  cluster: r15z1p1
  gpus: 256
  gpu_type: h100_80gb
integrations:
  - integration_type: git_repo
    git_repo: allenai/OLMo
    git_branch: train-olmo-large
    pip_install: -e .[train]
    ssh_clone: true
env_variables:
  PIP_DISABLE_PIP_VERSION_CHECK: "1"
  OMP_NUM_THREADS: "8"
  LOG_FILTER_TYPE: local_rank0_only
command: |-
  # Install AWS CLI (for download unsharded checkpoints).
  #apt-get update
  #apt-get install zip unzip
  #curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
  #unzip awscliv2.zip
  #sudo ./aws/install

  # Make sure we have a recent flash-attn.
  # NOTE: only pinning flash-attn here to future proof it.
  pip install flash-attn==2.5.3 --no-build-isolation
  pip install awscli

  # Show packages for debugging.
  pip freeze

  # Prepare environment.
  mkdir -p /root/.cache/torch
  # warm up huggingface cache
  pushd /root/.cache
  curl "https://storage.googleapis.com/dirkgr-public/huggingface_cache_v3.tar.gz" | tar -xzf -
  popd
  export HF_DATASETS_OFFLINE=1

  #checkpoint=s3://ai2-llm/checkpoints/OLMo-large/mitchish70-002/step35750-unsharded
  #mkdir /root/checkpoint-unsharded
  #aws s3 cp ${checkpoint}/config.yaml /root/checkpoint-unsharded/
  #aws s3 cp ${checkpoint}/train.pt /root/checkpoint-unsharded/
  #aws s3 cp ${checkpoint}/model.safetensors /root/checkpoint-unsharded/
  #aws s3 cp ${checkpoint}/optim.safetensors /root/checkpoint-unsharded/

  cd OLMo

  torchrun \
  --master_addr "$MASTER_ADDR" \
  --master_port "$MASTER_PORT" \
  --nnodes "$NUM_NODES" \
  --node_rank "$NODE_RANK" \
  --nproc_per_node 8 \
  scripts/train.py configs/mitchish70-s3.yaml \
    --run_name=mitchish70-002 \
    --wandb.group=mitchish70-official \
    '--load_path=${path.last_checkpoint:${remote_save_folder}}' \
    --global_train_batch_size=1536 \
    --device_train_microbatch_size=3 \
    --time_limit=604800 \
    --save_overwrite

#    '--load_path=${path.last_checkpoint:${remote_save_folder}}' \
#    --load_path=s3://ai2-llm/checkpoints/OLMo-large/mitchish70-002/step32310 \
#    --load_path_sharded_checkpointer=torch_new \
#    --load_path=s3://ai2-llm/checkpoints/OLMo-large/mitchish70-002/step32050-unsharded \
#    --load_path=s3://ai2-llm/checkpoints/OLMo-large/mitchish70-002/step32300-unsharded \
#    --load_path=s3://ai2-llm/checkpoints/OLMo-large/mitchish70-002/step32300 \
#    --load_path=s3://ai2-llm/checkpoints/OLMo-large/mitchish70-002/step34700-unsharded \
#    --load_path=/root/checkpoint-unsharded \
