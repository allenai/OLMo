name: olmo70b-from205000-300B
image: mosaicml/pytorch:2.2.1_cu121-python3.11-ubuntu20.04
# image: public.ecr.aws/z0f8p3z5/olmo:pytorch2.2.1_cu121-python3.11-ubuntu20.04
# image: us-central1-docker.pkg.dev/ai2-olmo/olmo/pytorch:2.2.1_cu121-python3.11-ubuntu20.04
scheduling:
  priority: auto
  # preemptible: true  # means it can be retried
  # max_retries: 10
compute:
  cluster: r15z4
  gpus: 896   # if you change this, change --nnodes below
  gpu_type: h100_80gb
  instance: oci.bm.gpu.h100.8
  # node_names:
integrations:
  - integration_type: git_repo
    git_repo: allenai/OLMo
    git_branch: dave/annealing
    pip_install: -e .[train]
    ssh_clone: true
  - integration_type: git_repo
    git_repo: allenai/OLMo-core
    git_branch: main
    pip_install: -e .
    ssh_clone: true
env_variables:
  PIP_DISABLE_PIP_VERSION_CHECK: "1"
  OMP_NUM_THREADS: "8"
  LOG_FILTER_TYPE: local_rank0_only
command: |-
  # Make sure we have a recent flash-attn.
  # NOTE: only pinning flash-attn here to future proof it.
  pip install flash-attn==2.5.3 --no-build-isolation
  # Install AWS CLI (for pre-downloading unsharded checkpoints).
  pip install awscli

  # Show packages for debugging.
  pip freeze

  # Prepare environment.
  mkdir -p /root/.cache/torch
  # warm up huggingface cache
  pushd /root/.cache
  curl "https://storage.googleapis.com/dirkgr-public/huggingface_cache_v3.tar.gz" | tar -xzf -
  popd
  export HF_DATASETS_OFFLINE=1

  cd OLMo

  echo "Launching train script..."
  torchrun \
  --nproc_per_node 8 \
  --nnodes 112:112 \
  --rdzv_id=21233 \
  --rdzv_backend=static \
  --rdzv_endpoint=$MASTER_ADDR:29400 \
  --node_rank=$NODE_RANK \
  --rdzv_conf="read_timeout=420" \
  scripts/train.py configs/annealing/olmo70b-from205000-300B.yaml \
    --load_path_sharded_checkpointer=olmo_core \
    --sharded_checkpointer=olmo_core \
    --global_train_batch_size=3584 \
    --device_train_microbatch_size=4 \
    --fsdp.sharding_strategy=HYBRID_SHARD \
    --fsdp.hybrid_sharding_num_model_replicas=4 \
    --time_limit=604800
