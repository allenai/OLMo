name: v1.7-fix_redpajama-step_2T-resume_optimizer-steps_50B
image: mosaicml/pytorch:2.1.2_cu121-python3.10-ubuntu20.04
compute:
  gpus: 256
  cluster: r15z4
  gpu_type: h100_80gb
  instance: oci.bm.gpu.h100.8
integrations:
  - integration_type: git_repo
    git_repo: allenai/OLMo
    git_branch: dave/annealing
    #git_commit: d765e8819f5b0be204c96b0b519de2372b0da729
    pip_install: -e .[train]
    ssh_clone: true
  - integration_type: git_repo
    git_repo: allenai/OLMo-core
    git_branch: main
    pip_install: -e .
    ssh_clone: true
env_variables:
  PIP_DISABLE_PIP_VERSION_CHECK: "1"
  OMP_NUM_THREADS: "8"
  LOG_FILTER_TYPE: local_rank0_only
command: |-
  # Make sure we have a recent flash-attn.
  # NOTE: only pinning flash-attn here to future proof it.
  pip install flash-attn==2.5.3 --no-build-isolation

  # Show packages for debugging.
  pip freeze

  # Prepare environment.
  mkdir -p /root/.cache/torch
  # warm up huggingface cache
  pushd /root/.cache
  curl "https://storage.googleapis.com/dirkgr-public/huggingface_cache_v3.tar.gz" | tar -xzf -
  popd
  export HF_DATASETS_OFFLINE=1

  cd OLMo
  
  torchrun \
  --master_addr $MASTER_ADDR \
  --master_port $MASTER_PORT \
  --nnodes $NUM_NODES \
  --node_rank $NODE_RANK \
  --nproc_per_node 8 \
    scripts/train.py configs/annealing/v1.7-fix_redpajama-step_2T-resume_optimizer-steps_50B.yaml \
      --model.flash_attention=true \
      --fsdp.wrapping_strategy=by_block_and_size \
      --fsdp.sharding_strategy=SHARD_GRAD_OP \
      --activation_checkpointing=fine_grained \
      --fused_loss=true \
      --device_train_microbatch_size=2 \
      --global_train_batch_size=1024 \
      --gen1_gc_interval=16
