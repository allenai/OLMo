<h1 align="center">AutoGPTQ</h1>
<p align="center">ä¸€ä¸ªåŸºäº GPTQ ç®—æ³•ï¼Œç®€å•æ˜“ç”¨ä¸”æ‹¥æœ‰ç”¨æˆ·å‹å¥½å‹æ¥å£çš„å¤§è¯­è¨€æ¨¡å‹é‡åŒ–å·¥å…·åŒ…ã€‚</p>
<p align="center">
    <a href="https://github.com/PanQiWei/AutoGPTQ/releases">
        <img alt="GitHub release" src="https://img.shields.io/github/release/PanQiWei/AutoGPTQ.svg">
    </a>
    <a href="https://pypi.org/project/auto-gptq/">
        <img alt="PyPI - Downloads" src="https://img.shields.io/pypi/dd/auto-gptq">
    </a>
</p>
<h4 align="center">
    <p>
        <a href="https://github.com/PanQiWei/AutoGPTQ/blob/main/README.md">English</a> |
        <b>ä¸­æ–‡</b>
    </p>
</h4>

*<center>ğŸ“£ å¥½ä¹…ä¸è§ï¼ğŸ‘‹ ä¸ƒæœˆå’Œå…«æœˆå°†ä¼šè¿æ¥æ¶æ„å‡çº§ï¼Œæ€§èƒ½ä¼˜åŒ–å’Œæ–°ç‰¹æ€§ï¼Œæ•¬è¯·å…³æ³¨ï¼ğŸ¥‚</center>*

## æ–°é—»æˆ–æ›´æ–°

- 2023-08-23 - (æ–°é—») - ğŸ¤— Transformersã€optimum å’Œ peft å®Œæˆäº†å¯¹ `auto-gptq` çš„é›†æˆï¼Œç°åœ¨ä½¿ç”¨ GPTQ æ¨¡å‹è¿›è¡Œæ¨ç†å’Œè®­ç»ƒå°†å˜å¾—æ›´å®¹æ˜“ï¼é˜…è¯» [è¿™ç¯‡åšå®¢](https://huggingface.co/blog/gptq-integration) å’Œç›¸å…³èµ„æºä»¥äº†è§£æ›´å¤šç»†èŠ‚ï¼
- 2023-08-21 - (æ–°é—») - é€šä¹‰åƒé—®å›¢é˜Ÿå‘å¸ƒäº†åŸºäº `auto-gptq` çš„ Qwen-7B 4bit é‡åŒ–ç‰ˆæœ¬æ¨¡å‹ï¼Œå¹¶æä¾›äº†[è¯¦å°½çš„æµ‹è¯„ç»“æœ](https://huggingface.co/Qwen/Qwen-7B-Chat-Int4#%E9%87%8F%E5%8C%96-quantization)
- 2023-08-06 - (æ›´æ–°) - æ”¯æŒ exllama çš„ q4 CUDA ç®—å­ä½¿å¾— int4 é‡åŒ–æ¨¡å‹èƒ½å¤Ÿè·å¾—è‡³å°‘1.3å€çš„æ¨ç†é€Ÿåº¦æå‡.
- 2023-08-04 - (æ›´æ–°) - æ”¯æŒ RoCm ä½¿å¾— AMD GPU çš„ç”¨æˆ·èƒ½å¤Ÿä½¿ç”¨ auto-gptq çš„ CUDA æ‹“å±•.
- 2023-07-26 - (æ›´æ–°) - ä¸€ä¸ªä¼˜é›…çš„ [PPL æµ‹è¯„è„šæœ¬](examples/benchmark/perplexity.py)ä»¥è·å¾—å¯ä»¥ä¸è¯¸å¦‚ `llama.cpp` ç­‰ä»£ç åº“è¿›è¡Œå…¬å¹³æ¯”è¾ƒçš„ç»“æœã€‚
- 2023-06-05 - (æ›´æ–°) - é›†æˆ ğŸ¤— peft æ¥ä½¿ç”¨ gptq é‡åŒ–è¿‡çš„æ¨¡å‹è®­ç»ƒé€‚åº”å±‚ï¼Œæ”¯æŒ LoRAï¼ŒAdaLoRAï¼ŒAdaptionPrompt ç­‰ã€‚
- 2023-05-30 - (æ›´æ–°) - æ”¯æŒä» ğŸ¤— Hub ä¸‹è½½é‡åŒ–å¥½çš„æ¨¡å‹æˆ–ä¸Šæ¬¡é‡åŒ–å¥½çš„æ¨¡å‹åˆ° ğŸ¤— Hubã€‚

*è·å–æ›´å¤šçš„å†å²ä¿¡æ¯ï¼Œè¯·è½¬è‡³[è¿™é‡Œ](docs/NEWS_OR_UPDATE.md)*

## æ€§èƒ½å¯¹æ¯”

### æ¨ç†é€Ÿåº¦
> ä»¥ä¸‹ç»“æœé€šè¿‡[è¿™ä¸ªè„šæœ¬](examples/benchmark/generation_speed.py)ç”Ÿæˆï¼Œæ–‡æœ¬è¾“å…¥çš„ batch size ä¸º1ï¼Œè§£ç ç­–ç•¥ä¸º beam search å¹¶ä¸”å¼ºåˆ¶æ¨¡å‹ç”Ÿæˆ512ä¸ª tokenï¼Œé€Ÿåº¦çš„è®¡é‡å•ä½ä¸º tokens/sï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰ã€‚
> 
> é‡åŒ–æ¨¡å‹é€šè¿‡èƒ½å¤Ÿæœ€å¤§åŒ–æ¨ç†é€Ÿåº¦çš„æ–¹å¼åŠ è½½ã€‚

| model         | GPU           | num_beams | fp16  | gptq-int4 |
|---------------|---------------|-----------|-------|-----------|
| llama-7b      | 1xA100-40G    | 1         | 18.87 | 25.53     |
| llama-7b      | 1xA100-40G    | 4         | 68.79 | 91.30     |
| moss-moon 16b | 1xA100-40G    | 1         | 12.48 | 15.25     |
| moss-moon 16b | 1xA100-40G    | 4         | OOM   | 42.67     |
| moss-moon 16b | 2xA100-40G    | 1         | 06.83 | 06.78     |
| moss-moon 16b | 2xA100-40G    | 4         | 13.10 | 10.80     |
| gpt-j 6b      | 1xRTX3060-12G | 1         | OOM   | 29.55     |
| gpt-j 6b      | 1xRTX3060-12G | 4         | OOM   | 47.36     |


### å›°æƒ‘åº¦ï¼ˆPPLï¼‰
å¯¹äºå›°æƒ‘åº¦çš„å¯¹æ¯”ï¼Œ ä½ å¯ä»¥å‚è€ƒ [è¿™é‡Œ](https://github.com/qwopqwop200/GPTQ-for-LLaMa#result) å’Œ [è¿™é‡Œ](https://github.com/qwopqwop200/GPTQ-for-LLaMa#gptq-vs-bitsandbytes)

## å®‰è£…

### å¿«é€Ÿå®‰è£…
ä½ å¯ä»¥é€šè¿‡ pip æ¥å®‰è£…ä¸ PyTorch 2.0.1 ç›¸å…¼å®¹çš„æœ€æ–°ç¨³å®šç‰ˆæœ¬çš„ AutoGPTQ çš„é¢„æ„å»ºè½®å­æ–‡ä»¶ï¼š

* å¯¹äº CUDA 11.7ï¼š `pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu117/`
* å¯¹äº CUDA 11.8ï¼š `pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/`
* å¯¹äº RoCm 5.4.2ï¼š `pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/rocm542/`

**è­¦å‘Šï¼š** é¢„æ„å»ºçš„è½®å­æ–‡ä»¶ä¸ä¸€å®šåœ¨ PyTorch çš„ nightly ç‰ˆæœ¬ä¸Šæœ‰æ•ˆã€‚å¦‚æœè¦ä½¿ç”¨ PyTorch çš„ nightly ç‰ˆæœ¬ï¼Œè¯·ä»æºç å®‰è£… AutoGPTQã€‚

#### å–æ¶ˆ cuda æ‹“å±•çš„å®‰è£…
é»˜è®¤æƒ…å†µä¸‹ï¼Œåœ¨ `torch` å’Œ `cuda` å·²ç»äºä½ çš„æœºå™¨ä¸Šè¢«å®‰è£…æ—¶ï¼Œcuda æ‹“å±•å°†è¢«è‡ªåŠ¨å®‰è£…ï¼Œå¦‚æœä½ ä¸æƒ³è¦è¿™äº›æ‹“å±•çš„è¯ï¼Œé‡‡ç”¨ä»¥ä¸‹å®‰è£…å‘½ä»¤ï¼š
```shell
BUILD_CUDA_EXT=0 pip install auto-gptq
```
åŒæ—¶ä¸ºç¡®ä¿è¯¥æ‹“å±•â€”â€”`autogptq_cuda` ä¸å†å­˜åœ¨äºä½ çš„è™šæ‹Ÿç¯å¢ƒï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š
```shell
pip uninstall autogptq_cuda -y
```

#### æ”¯æŒä½¿ç”¨ triton åŠ é€Ÿ
è‹¥æƒ³ä½¿ç”¨ `triton` åŠ é€Ÿæ¨¡å‹æ¨ç†ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š
> è­¦å‘Šï¼šç›®å‰ triton ä»…æ”¯æŒ linux æ“ä½œç³»ç»Ÿï¼›å½“ä½¿ç”¨ triton æ—¶ 3-bit æ•°å€¼ç±»å‹çš„é‡åŒ–å°†ä¸è¢«æ”¯æŒ

```shell
pip install auto-gptq[triton]
```

### ä»æºç å®‰è£…
<details>
<summary>ç‚¹å‡»ä»¥æŸ¥çœ‹è¯¦æƒ…</summary>

å…‹éš†æºç :
```shell
git clone https://github.com/PanQiWei/AutoGPTQ.git && cd AutoGPTQ
```
ç„¶åï¼Œä»é¡¹ç›®ç›®å½•å®‰è£…:
```shell
pip install .
```
æ­£å¦‚åœ¨å¿«é€Ÿå®‰è£…ä¸€èŠ‚ï¼Œä½ å¯ä»¥ä½¿ç”¨ `BUILD_CUDA_EXT=0` æ¥å–æ¶ˆæ„å»º cuda æ‹“å±•ã€‚

å¦‚æœä½ æƒ³è¦ä½¿ç”¨ triton åŠ é€Ÿä¸”å…¶èƒ½å¤Ÿè¢«ä½ çš„æ“ä½œç³»ç»Ÿæ‰€æ”¯æŒï¼Œè¯·ä½¿ç”¨ `.[triton]`ã€‚

å¯¹åº” AMD GPUsï¼Œä¸ºäº†ä»æºç å®‰è£…ä»¥æ”¯æŒ RoCmï¼Œè¯·è®¾ç½® `ROCM_VERSION` ç¯å¢ƒå˜é‡ã€‚åŒæ—¶é€šè¿‡è®¾ç½® `PYTORCH_ROCM_ARCH` ([reference](https://github.com/pytorch/pytorch/blob/7b73b1e8a73a1777ebe8d2cd4487eb13da55b3ba/setup.py#L132)) å¯æå‡ç¼–è¯‘é€Ÿåº¦ï¼Œä¾‹å¦‚ï¼šå¯¹äº MI200 ç³»åˆ—è®¾å¤‡ï¼Œè¯¥å˜é‡å¯è®¾ä¸º `gfx90a`ã€‚ä¾‹å­ï¼š

```
ROCM_VERSION=5.6 pip install .
```

å¯¹äº RoCm ç³»ç»Ÿï¼Œåœ¨ä»æºç å®‰è£…æ—¶é¢å¤–éœ€è¦æå‰å®‰è£…ä»¥ä¸‹åŒ…ï¼š`rocsparse-dev`, `hipsparse-dev`, `rocthrust-dev`, `rocblas-dev` and `hipblas-dev`ã€‚

</details>

## å¿«é€Ÿå¼€å§‹

### é‡åŒ–å’Œæ¨ç†
> è­¦å‘Šï¼šè¿™é‡Œä»…æ˜¯å¯¹ AutoGPTQ ä¸­åŸºæœ¬æ¥å£çš„ç”¨æ³•å±•ç¤ºï¼Œåªä½¿ç”¨äº†ä¸€æ¡æ–‡æœ¬æ¥é‡åŒ–ä¸€ä¸ªç‰¹åˆ«å°çš„æ¨¡å‹ï¼Œå› æ­¤å…¶ç»“æœçš„è¡¨ç°å¯èƒ½ä¸å¦‚åœ¨å¤§æ¨¡å‹ä¸Šæ‰§è¡Œé‡åŒ–åé¢„æœŸçš„é‚£æ ·å¥½ã€‚

ä»¥ä¸‹å±•ç¤ºäº†ä½¿ç”¨ `auto_gptq` è¿›è¡Œé‡åŒ–å’Œæ¨ç†çš„æœ€ç®€å•ç”¨æ³•ï¼š
```python
from transformers import AutoTokenizer, TextGenerationPipeline
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig


pretrained_model_dir = "facebook/opt-125m"
quantized_model_dir = "opt-125m-4bit"


tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, use_fast=True)
examples = [
    tokenizer(
        "auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm."
    )
]

quantize_config = BaseQuantizeConfig(
    bits=4,  # å°†æ¨¡å‹é‡åŒ–ä¸º 4-bit æ•°å€¼ç±»å‹
    group_size=128,  # ä¸€èˆ¬æ¨èå°†æ­¤å‚æ•°çš„å€¼è®¾ç½®ä¸º 128
    desc_act=False,  # è®¾ä¸º False å¯ä»¥æ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦ï¼Œä½†æ˜¯ ppl å¯èƒ½ä¼šè½»å¾®åœ°å˜å·®
)

# åŠ è½½æœªé‡åŒ–çš„æ¨¡å‹ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹æ€»æ˜¯ä¼šè¢«åŠ è½½åˆ° CPU å†…å­˜ä¸­
model = AutoGPTQForCausalLM.from_pretrained(pretrained_model_dir, quantize_config)

# é‡åŒ–æ¨¡å‹, æ ·æœ¬çš„æ•°æ®ç±»å‹åº”è¯¥ä¸º List[Dict]ï¼Œå…¶ä¸­å­—å…¸çš„é”®æœ‰ä¸”ä»…æœ‰ input_ids å’Œ attention_mask
model.quantize(examples)

# ä¿å­˜é‡åŒ–å¥½çš„æ¨¡å‹
model.save_quantized(quantized_model_dir)

# ä½¿ç”¨ safetensors ä¿å­˜é‡åŒ–å¥½çš„æ¨¡å‹
model.save_quantized(quantized_model_dir, use_safetensors=True)

# å°†é‡åŒ–å¥½çš„æ¨¡å‹ç›´æ¥ä¸Šä¼ è‡³ Hugging Face Hub 
# å½“ä½¿ç”¨ use_auth_token=True æ—¶, ç¡®ä¿ä½ å·²ç»é¦–å…ˆä½¿ç”¨ huggingface-cli login è¿›è¡Œäº†ç™»å½•
# æˆ–è€…å¯ä»¥ä½¿ç”¨ use_auth_token="hf_xxxxxxx" æ¥æ˜¾å¼åœ°æ·»åŠ è´¦æˆ·è®¤è¯ token
# ï¼ˆå–æ¶ˆä¸‹é¢ä¸‰è¡Œä»£ç çš„æ³¨é‡Šæ¥ä½¿ç”¨è¯¥åŠŸèƒ½ï¼‰
# repo_id = f"YourUserName/{quantized_model_dir}"
# commit_message = f"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}"
# model.push_to_hub(repo_id, commit_message=commit_message, use_auth_token=True)

# æˆ–è€…ä½ ä¹Ÿå¯ä»¥åŒæ—¶å°†é‡åŒ–å¥½çš„æ¨¡å‹ä¿å­˜åˆ°æœ¬åœ°å¹¶ä¸Šä¼ è‡³ Hugging Face Hub
# ï¼ˆå–æ¶ˆä¸‹é¢ä¸‰è¡Œä»£ç çš„æ³¨é‡Šæ¥ä½¿ç”¨è¯¥åŠŸèƒ½ï¼‰
# repo_id = f"YourUserName/{quantized_model_dir}"
# commit_message = f"AutoGPTQ model for {pretrained_model_dir}: {quantize_config.bits}bits, gr{quantize_config.group_size}, desc_act={quantize_config.desc_act}"
# model.push_to_hub(repo_id, save_dir=quantized_model_dir, use_safetensors=True, commit_message=commit_message, use_auth_token=True)

# åŠ è½½é‡åŒ–å¥½çš„æ¨¡å‹åˆ°èƒ½è¢«è¯†åˆ«åˆ°çš„ç¬¬ä¸€å—æ˜¾å¡ä¸­
model = AutoGPTQForCausalLM.from_quantized(quantized_model_dir, device="cuda:0")

# ä» Hugging Face Hub ä¸‹è½½é‡åŒ–å¥½çš„æ¨¡å‹å¹¶åŠ è½½åˆ°èƒ½è¢«è¯†åˆ«åˆ°çš„ç¬¬ä¸€å—æ˜¾å¡ä¸­
# model = AutoGPTQForCausalLM.from_quantized(repo_id, device="cuda:0", use_safetensors=True, use_triton=False)

# ä½¿ç”¨ model.generate æ‰§è¡Œæ¨ç†
print(tokenizer.decode(model.generate(**tokenizer("auto_gptq is", return_tensors="pt").to(model.device))[0]))

# æˆ–è€…ä½¿ç”¨ TextGenerationPipeline
pipeline = TextGenerationPipeline(model=model, tokenizer=tokenizer)
print(pipeline("auto-gptq is")[0]["generated_text"])
```

å‚è€ƒ [æ­¤æ ·ä¾‹è„šæœ¬](examples/quantization/quant_with_alpaca.py) ä»¥äº†è§£è¿›é˜¶çš„ç”¨æ³•ã€‚

### è‡ªå®šä¹‰æ¨¡å‹

<details>

<summary>ä»¥ä¸‹å±•ç¤ºäº†å¦‚ä½•æ‹“å±• `auto_gptq` ä»¥æ”¯æŒ `OPT` æ¨¡å‹ï¼Œå¦‚ä½ æ‰€è§ï¼Œè¿™éå¸¸ç®€å•ï¼š</summary>

```python
from auto_gptq.modeling import BaseGPTQForCausalLM


class OPTGPTQForCausalLM(BaseGPTQForCausalLM):
    # chained attribute name of transformer layer block
    layers_block_name = "model.decoder.layers"
    # chained attribute names of other nn modules that in the same level as the transformer layer block
    outside_layer_modules = [
        "model.decoder.embed_tokens", "model.decoder.embed_positions", "model.decoder.project_out",
        "model.decoder.project_in", "model.decoder.final_layer_norm"
    ]
    # chained attribute names of linear layers in transformer layer module
    # normally, there are four sub lists, for each one the modules in it can be seen as one operation, 
    # and the order should be the order when they are truly executed, in this case (and usually in most cases), 
    # they are: attention q_k_v projection, attention output projection, MLP project input, MLP project output
    inside_layer_modules = [
        ["self_attn.k_proj", "self_attn.v_proj", "self_attn.q_proj"],
        ["self_attn.out_proj"],
        ["fc1"],
        ["fc2"]
    ]
```
ç„¶å, ä½ å°±å¯ä»¥åƒåœ¨åŸºæœ¬ç”¨æ³•ä¸€èŠ‚ä¸­å±•ç¤ºçš„é‚£æ ·ä½¿ç”¨ `OPTGPTQForCausalLM.from_pretrained` å’Œå…¶ä»–æ–¹æ³•ã€‚

</details>


### åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šæ‰§è¡Œè¯„ä¼°
ä½ å¯ä»¥ä½¿ç”¨åœ¨ `auto_gptq.eval_tasks` ä¸­å®šä¹‰çš„ä»»åŠ¡æ¥è¯„ä¼°é‡åŒ–å‰åçš„æ¨¡å‹åœ¨æŸä¸ªç‰¹å®šä¸‹æ¸¸ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚

è¿™äº›é¢„å®šä¹‰çš„æ¨¡å‹æ”¯æŒæ‰€æœ‰åœ¨ [ğŸ¤— transformers](https://github.com/huggingface/transformers)å’Œæœ¬é¡¹ç›®ä¸­è¢«å®ç°äº†çš„ causal-language-modelsã€‚

<details>

<summary>ä»¥ä¸‹æ˜¯ä½¿ç”¨ `cardiffnlp/tweet_sentiment_multilingual` æ•°æ®é›†åœ¨åºåˆ—åˆ†ç±»ï¼ˆæ–‡æœ¬åˆ†ç±»ï¼‰ä»»åŠ¡ä¸Šè¯„ä¼° `EleutherAI/gpt-j-6b` æ¨¡å‹çš„ç¤ºä¾‹:</summary>

```python
from functools import partial

import datasets
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig

from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
from auto_gptq.eval_tasks import SequenceClassificationTask


MODEL = "EleutherAI/gpt-j-6b"
DATASET = "cardiffnlp/tweet_sentiment_multilingual"
TEMPLATE = "Question:What's the sentiment of the given text? Choices are {labels}.\nText: {text}\nAnswer:"
ID2LABEL = {
    0: "negative",
    1: "neutral",
    2: "positive"
}
LABELS = list(ID2LABEL.values())


def ds_refactor_fn(samples):
    text_data = samples["text"]
    label_data = samples["label"]

    new_samples = {"prompt": [], "label": []}
    for text, label in zip(text_data, label_data):
        prompt = TEMPLATE.format(labels=LABELS, text=text)
        new_samples["prompt"].append(prompt)
        new_samples["label"].append(ID2LABEL[label])

    return new_samples


#  model = AutoModelForCausalLM.from_pretrained(MODEL).eval().half().to("cuda:0")
model = AutoGPTQForCausalLM.from_pretrained(MODEL, BaseQuantizeConfig())
tokenizer = AutoTokenizer.from_pretrained(MODEL)

task = SequenceClassificationTask(
        model=model,
        tokenizer=tokenizer,
        classes=LABELS,
        data_name_or_path=DATASET,
        prompt_col_name="prompt",
        label_col_name="label",
        **{
            "num_samples": 1000,  # how many samples will be sampled to evaluation
            "sample_max_len": 1024,  # max tokens for each sample
            "block_max_len": 2048,  # max tokens for each data block
            # function to load dataset, one must only accept data_name_or_path as input 
            # and return datasets.Dataset
            "load_fn": partial(datasets.load_dataset, name="english"),  
            # function to preprocess dataset, which is used for datasets.Dataset.map, 
            # must return Dict[str, list] with only two keys: [prompt_col_name, label_col_name]
            "preprocess_fn": ds_refactor_fn,  
            # truncate label when sample's length exceed sample_max_len
            "truncate_prompt": False  
        }
    )

# note that max_new_tokens will be automatically specified internally based on given classes
print(task.run())

# self-consistency
print(
    task.run(
        generation_config=GenerationConfig(
            num_beams=3,
            num_return_sequences=3,
            do_sample=True
        )
    )
)
```

</details>

## äº†è§£æ›´å¤š
[æ•™ç¨‹](docs/tutorial) æä¾›äº†å°† `auto_gptq` é›†æˆåˆ°ä½ çš„é¡¹ç›®ä¸­çš„æ‰‹æŠŠæ‰‹æŒ‡å¯¼å’Œæœ€ä½³å®è·µå‡†åˆ™ã€‚

[ç¤ºä¾‹](examples/README.md) æä¾›äº†å¤§é‡ç¤ºä¾‹è„šæœ¬ä»¥å°† `auto_gptq` ç”¨äºä¸åŒé¢†åŸŸã€‚

## æ”¯æŒçš„æ¨¡å‹

> ä½ å¯ä»¥ä½¿ç”¨ `model.config.model_type` æ¥å¯¹ç…§ä¸‹è¡¨ä»¥æ£€æŸ¥ä½ æ­£åœ¨ä½¿ç”¨çš„ä¸€ä¸ªæ¨¡å‹æ˜¯å¦è¢« `auto_gptq` æ‰€æ”¯æŒã€‚
> 
> æ¯”å¦‚ï¼Œ `WizardLM`ï¼Œ`vicuna` å’Œ `gpt4all` æ¨¡å‹çš„ `model_type` çš†ä¸º `llama`ï¼Œ å› æ­¤è¿™äº›æ¨¡å‹çš†è¢« `auto_gptq` æ‰€æ”¯æŒã€‚

| model type                         | quantization | inference | peft-lora | peft-ada-lora | peft-adaption_prompt                                                              |
|------------------------------------|--------------|-----------|-----------|---------------|-----------------------------------------------------------------------------------|
| bloom                              | âœ…            | âœ…         | âœ…         | âœ…             |                                                                                   |
| gpt2                               | âœ…            | âœ…         | âœ…         | âœ…             |                                                                                   |
| gpt_neox                           | âœ…            | âœ…         | âœ…         | âœ…             | âœ…[è¦æ±‚è¯¥åˆ†æ”¯çš„ peft](https://github.com/PanQiWei/peft/tree/multi_modal_adaption_prompt) |
| gptj                               | âœ…            | âœ…         | âœ…         | âœ…             | âœ…[è¦æ±‚è¯¥åˆ†æ”¯çš„ peft](https://github.com/PanQiWei/peft/tree/multi_modal_adaption_prompt) |
| llama                              | âœ…            | âœ…         | âœ…         | âœ…             | âœ…                                                                                 |
| moss                               | âœ…            | âœ…         | âœ…         | âœ…             | âœ…[è¦æ±‚è¯¥åˆ†æ”¯çš„ peft](https://github.com/PanQiWei/peft/tree/multi_modal_adaption_prompt) |
| opt                                | âœ…            | âœ…         | âœ…         | âœ…             |                                                                                   |
| gpt_bigcode                        | âœ…            | âœ…         | âœ…         | âœ…             |                                                                                   |
| codegen                            | âœ…            | âœ…         | âœ…         | âœ…             |                                                                                   |
| falcon(RefinedWebModel/RefinedWeb) | âœ…            | âœ…         | âœ…         | âœ…             |                                                                                   |

## æ”¯æŒçš„è¯„ä¼°ä»»åŠ¡
ç›®å‰ï¼Œ `auto_gptq` æ”¯æŒä»¥ä¸‹è¯„ä¼°ä»»åŠ¡ï¼š `LanguageModelingTask`, `SequenceClassificationTask` å’Œ `TextSummarizationTask`ï¼›æ›´å¤šçš„è¯„ä¼°ä»»åŠ¡å³å°†åˆ°æ¥ï¼

## è‡´è°¢
- ç‰¹åˆ«æ„Ÿè°¢ **Elias Frantar**ï¼Œ **Saleh Ashkboos**ï¼Œ **Torsten Hoefler** å’Œ **Dan Alistarh** æå‡º **GPTQ** ç®—æ³•å¹¶å¼€æº[ä»£ç ](https://github.com/IST-DASLab/gptq)ã€‚
- ç‰¹åˆ«æ„Ÿè°¢ **qwopqwop200**ï¼Œ æœ¬é¡¹ç›®ä¸­æ¶‰åŠåˆ°æ¨¡å‹é‡åŒ–çš„ä»£ç ä¸»è¦å‚è€ƒè‡ª [GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa/tree/cuda)ã€‚

[![Star History Chart](https://api.star-history.com/svg?repos=PanQiwei/AutoGPTQ&type=Date)](https://star-history.com/#PanQiWei/AutoGPTQ&Date)